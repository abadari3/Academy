<html>
  {{template "header.tmpl.html"}}
<body>
  {{template "nav.tmpl.html"}}

<br>
<div class="container" style="padding: 20px;">
  <!-- <div class="alert alert-info text-center" role="alert">
      To deploy your own copy, and learn the fundamentals of the Heroku platform, head over to the <a href="https://devcenter.heroku.com/articles/getting-started-with-go" class="alert-link">Getting Started with Go on Heroku</a> tutorial.
  </div> -->
  <h1 style="text-align: center;">What Can Be Done?</h1>
  <div class="row">
    <div class="">
      <!-- <h3><span class="glyphicon glyphicon-link"></span> Next Steps</h3> -->
      <p>
        Solving bias in computing is a complex issue, such as is solving bias in human life, many things can be done to strive for that eventual unbiased ideal.<br><br>
        The most important part of AI systems to help detect and solve bias is transparency. In the example of Amazon’s recruiting AI, or Northpointe’s bail AI, both companies kept their technologies secret and undisclosed. While this makes sense for their economic bottom line, it also makes detection of biases that much harder. When these systems are used in places like criminal justice, transparency about what goes into making these decisions can mean the difference between life and death.<br><br>
        IBM has released AI Fairness 360, which is an open-source metric toolkit to check ML models and their datasets for biases. The toolkit contains nine algorithms to both detect and mitigate biases like racial, and gender discrimination, and other disparate impacts. The fact that this toolkit is open source is another step in the right direction towards openness and transparency in the AI community.<br><br>
        Similarly, Google has released four general guidelines for creating responsible AI. First, is to design the model with fairness and inclusion in mind. Then, using representative datasets to train the model, as well as working with social scientists to ensure that data is being portrayed accurately helps remove some implicit data bias before it is passed to the model. Once the model is created, checking the end result for biases is another important practice. Unintended consequences are to be expected with something like AI and creating metrics to back-test the model is a good way to ensure it is behaving fairly.<br><br>
        Other things that can be done are to identify the critical systems that require a human’s confirmation for decision making, in order to increase accountability. Standardizing guidelines, rules, and regulations concerning AI bias, as well as special research into the disparate impact that algorithms can produce actionable results that reduce the bias built into AI systems.<br><br>
        Although large corporations like IBM and Google have set out their AI fairness practices, mitigation of these biases so far has been an unsolved question. A lack of education about the issue, and willingness to sweep it under the rug in favor of performance, combined with the lack of diversity in computer scientists, and with datasets, creates many opportunities for bias to seep into AI systems. Without research about what biases are present in society, how these biases are reflected both implicitly and explicitly, and how to remove the causes of bias in these systems will continue to perpetuate bias without regulation. Although research is starting to be done, and companies are starting to pay attention, the question of stopping biased artificial intelligence algorithms will to be unsolved as long as humans remain biased fundamentally.<br><br>
      </p>
      <h3><span class="glyphicon glyphicon-link"></span> References</h3>
      <ul>
        <li>Cathy ONeil. 2017. Weapons of math destruction: how big data increases inequality and threatens democracy, Great Britain: Penguin Books.</li>
        <li>IBM. 2019. Introducing AI Fairness 360, A Step Towards Trusted AI — IBM Research. (February 2019). Retrieved from <a href="https://www.ibm.com/blogs/research/2018/09/ai-fairness-360/">https://www.ibm.com/blogs/research/2018/09/ai-fairness-360/</a></li>
        <li>Google. Responsible AI Practices. Retrieved from <a href="https://ai.google/responsibilities/responsible-ai-practices/?category=fairness">https://ai.google/responsibilities/responsible-ai-practices/?category=fairness</a></li>
      </ul>
    </div>
  </div> <!-- row -->
</div>

</body>
</html>
