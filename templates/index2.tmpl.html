<html>
  {{template "header.tmpl.html"}}
<body>
  {{template "nav.tmpl.html"}}

<br>
<div class="container" style="padding: 20px;">
  <!-- <div class="alert alert-info text-center" role="alert">
      To deploy your own copy, and learn the fundamentals of the Heroku platform, head over to the <a href="https://devcenter.heroku.com/articles/getting-started-with-go" class="alert-link">Getting Started with Go on Heroku</a> tutorial.
  </div> -->
  <h1 style="text-align: center;">How does Bias look like in AI?</h1>
  <div class="row">
    <div class="">
      <!-- <h3><span class="glyphicon glyphicon-link"></span> Next Steps</h3> -->
      <p>
        First, we will explore how AI algorithms are currently being used to reduce inherent bias, to show the benefits AI can have over regular human decision-making. Then, we can explore the different kinds of biases that arise from such systems.<br><br>
        AI can help reduce bias, since an AI agent can process data without a “human’s subjective interpretation of the data”. Research has shown that AI helps eliminate some unconscious biases that goes into court decisions, job interviews, and other personal judgement decisions. Thus human racial or gender bias is not directly influencing decision making.<br><br>
        For example, one study decided to test bail decisions of an AI model against judge’s decisions, and they were able to make much better predictions than human judges could, in one case reducing jailing rates by up to 42% without any corresponding increase in crime rates, while simultaneously reducing racial disparities. The implications of such systems are huge, using AI models to make decisions much more quickly and accurately than a human counterpart, allows for a less bureaucratic and more efficient criminal justice system.<br><br>
        However, many times these same systems that are put into place to reduce bias end up using other information as a proxy for different biases. For example, one system used to help judges make bail decisions used address as a proxy for race, picking up on inherent societal biases [5]. Even if we tell the model to disregard race, the model may pick up on the inherent biases in the training data. If a judge makes a bad decision in court, it is easy to hold the judge accountable for his bad decision, but when an abstract AI algorithm perpetuates biases, it can be very hard to argue against it. Unlike humans, AI carries the power to make decisions, without the burden of responsibility.<br><br>
        Typically, machine learning systems are black boxes, so the people implementing them into complete AI systems may not have an intimate understanding of how they work, and what biases they perpetuate. These things may not be so bad when these systems are deciding what shows Netflix wants to recommend us, they become much more important and dangerous when these AI systems are deciding our bank interest, investment strategy, and making court decisions.<br><br>
        So why do we care? Whether we like it or not, AI algorithms will continue to be implemented, and the only way to make sure these biases aren’t perpetuated unchecked by these algorithms is to build systems that regulate and give feedback to correct for these biases.<br><br>
        Creation of an AI system consists of three major stages: framing the problem, collecting the data, processing the data. The first step for any AI system is to frame what exactly it will do. This could mean quantifying vague variables and defining variables or functions to optimize. Bias is introduced when the framing of the problem is not complete or clear. Consider an AI system being created to make investment decisions for a bank, and its goal is to maximize profits. It could soon find itself at an optimal solution, selling subprime mortgage backed securities to unwitting investors, even if being unethical is not what the bank intended. This quite literally happened before 2008, as many of the algorithms created to assess risk were based on flawed models and failed to react before the housing market crashed.<br><br>
        Fixing biases introduced during this stage would mean modifying the functionality of the underlying model, because it’s the model itself that is flawed.<br><br>
        Biases introduced in the second stage, data collection, is likely due to training data. There are two ways that training data can be biased: the data is unrepresentative, or the data itself is biased. If the data is unrepresentative of the target population, then the model is not trained in underrepresented cases. Face recognition systems were trained with mainly white faces and is thus worse at recognizing faces of other races. One example of the data itself being biased, is Amazon’s recruiting AI preferring men to women, because historical data of hiring decisions reflect those existing gender biases. Thus, the hiring algorithm unintentionally perpetuated genderism in an already male-dominated industry.<br><br>
        Bias introduced in the third stage, processing data, stems from drawing flawed conclusions from raw data. Even if the problem is framed correctly, and the data is perfect, if the interpretation of these results is biased, then the end result will also be biased. Measuring the accuracy of different processing implantations is easy, while trying to find out how biased is a certain implementation is difficult.<br><br>
        The reason this is an open question, is because AI bias is difficult to find and correct. AI implementation is not widespread and is still in its relative infancy. Bias is hard to detect, because bias can only be seen during post-analysis of a model’s predictions. Thus, the nature of algorithmic bias in artificial intelligence is ever shifting and will reveal itself as we understand our own biases, and how they are reflected in our model’s implementation.<br><br>
      </p>
      <h3><span class="glyphicon glyphicon-link"></span> References</h3>
      <ul>
        <li>Jake Silberg and James Manyika. 2020. Tackling bias in artificial intelligence (and in humans). (July 2020). Retrieved from <a href="https://www.mckinsey.com/featured-insights/artificial-intelligence/tackling-bias-in-artificial-intelligence-and-in-humans#">https://www.mckinsey.com/featured-insights/artificial-intelligence/tackling-bias-in-artificial-intelligence-and-in-humans#</a></li>
        <li>Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, and Sendhil Mullainathan. 2017. Human Decisions and Machine Predictions*. The Quarterly Journal of Economics (2017). DOI:<a href="http://dx.doi.org/10.1093/qje/qjx032">http://dx.doi.org/10.1093/qje/qjx032</a></li>
        <li>Shara Tonn. 2019. Can AI help judges make the bail system fairer and safer? (March 2019). Retrieved from <a href="https://engineering.stanford.edu/magazine/article/can-ai-help-judges-make-bail-system-fairer-and-safer">https://engineering.stanford.edu/magazine/article/can-ai-help-judges-make-bail-system-fairer-and-safer</a></li>
        <li>Karen Hao. 2020. This is how AI bias really happens-and why it’s so hard to fix. (April 2020). Retrieved from <a href="https://www.technologyreview.com/2019/02/04/137602/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/">https://www.technologyreview.com/2019/02/04/137602/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/</a></li>
        <li>Osonde A. Osoba and William I.V. Welser. 2017. The Risks of Bias and Errors in Artificial Intelligence. (April 2017). Retrieved from <a href="https://www.rand.org/pubs/research_reports/RR1744.html">https://www.rand.org/pubs/research_reports/RR1744.html</a></li>
        <li>Jeffrey Dastin. 2018. Amazon scraps secret AI recruiting tool that showed bias against women. (October 2018). Retrieved from <a href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G">https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G</a></li>
        <li>Richmond Alake. 2020. Algorithm Bias In Artificial Intelligence Needs To Be Discussed (And Addressed). (April 2020). Retrieved from <a href="https://towardsdatascience.com/algorithm-bias-in-artificial-intelligence-needs-to-be-discussed-and-addressed-8d369d675a70">https://towardsdatascience.com/algorithm-bias-in-artificial-intelligence-needs-to-be-discussed-and-addressed-8d369d675a70</a></li>
      </ul>
    </div>
  </div> <!-- row -->
</div>

</body>
</html>
