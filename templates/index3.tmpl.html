<html>
  {{template "header.tmpl.html"}}
<body>
  {{template "nav.tmpl.html"}}

<br>
<div class="container" style="padding: 20px;">
  <!-- <div class="alert alert-info text-center" role="alert">
      To deploy your own copy, and learn the fundamentals of the Heroku platform, head over to the <a href="https://devcenter.heroku.com/articles/getting-started-with-go" class="alert-link">Getting Started with Go on Heroku</a> tutorial.
  </div> -->
  <h1 style="text-align: center;">How Do We Detect AI Bias?</h1>
  <div class="row">
    <div class="">
      <!-- <h3><span class="glyphicon glyphicon-link"></span> Next Steps</h3> -->
      <p>
        A lot of biases that are reflected in current society, like genderism or racism, reflect themselves in explicit ways and implicit ways. When Amazon learned that the AI was discriminating against women, they edited the code to make them neutral to gendered words. There was still “no guarantee that the machines would not devise other ways of sorting candidates that could prove discriminatory”. Amazon’s AI then started to pick up on implicitly gendered words, certain words that were more correlated with men over women, while not explicitly being gendered. This led Amazon to shut down this AI, due to the biases which they were unable to solve.<br><br>
        Also, many biases are not recognized until you’ve analyzed the outputted data from the model and compared to unbiased expectations. Once biases are recognized, it can be hard to identify what is causing the bias, and then how to get rid of it. The number of unknowns makes it impossible to adequately create a bias-free system.<br><br>
        One way to help mitigate biases that arise from this kind, is transparency about processes and metrics that helps external investigators understand the attributes considered by the artificial intelligence [1]. Other strategies could be to use internal teams, as well as third parties to audit both data and models. This helps transparency and helps make sure that biased models have some infrastructure for accountability.<br><br>
        Another reason is because many AI systems aren’t built with the purpose of bias detection in mind, but rather for purely performance. When doing Deep Learning for instance, the same kind of data is used for both training and testing, thus the underlying data will have bias that helps portray itself as more accurate, due to this underlying biased data. When unbiased data is fed into the system, the system makes the same biased conclusions, since the bias-detection features are baked into the neural net.<br><br>
        AI bias can be so pervasive due to computing’s inclination to build modular, multi-use systems. Abstraction and modular design are seen as key pillars to good systems and helps reduce a lot of work for computer programmers. However, these same pillars can be “dangerously misguided when they enter the societal context that surrounds decision-making systems”.<br><br>
        A lack of social context when creating AI systems can have unintended effects, due to their deployment in different social environments. One community’s definition of a “fair” court decision could be different than another’s, and if these decisions are being made by an AI system, these biases can pervade in a systematic way. If a modular system trained for and designed to do one task, gets modified to do another task, then the lack of context to the model leads to ignorance of the biases its propagating.<br><br>
        Basically, different people have different definitions of what is fair and what is biased. Does being fair mean equity, where everyone is treated the same, or equality, which gives proportional representation and opportunity?<br><br>
        An AI algorithm to make bail and sentencing decisions, called COMPAS, was labelled as being biased against African Americans, but the creators of the algorithm, among all ten risk levels of defendants, proved that there was “approximate equality between black defendants”. ProPublica claimed that the model was biased, because among non-reoffending defendants , the AI was twice as likely to classify black people as high-risk, compared to white people. Is it fair or not? Any AI cannot be mathematically fair in both of those measures. We still don’t know for sure, because the company that created it, Northpointe, has “to disclose the details of its proprietary algorithm, making it impossible to fully assess the extent to which it may be unfair.”<br><br>
        This is the key that makes the question of detecting and fixing bias in AI systems an unsolvable one. Creation of fair AI first necessitates the careful definition of “fair,” and two different AI’s can have completely mutually exclusive definitions of fair. Deep philosophical questions like “what is fairness” cannot be answered in simple terms, context matters a lot.<br><br>
      </p>
      <h3><span class="glyphicon glyphicon-link"></span> References</h3>
      <ul>
        <li>Cathy ONeil. 2017. Weapons of math destruction: how big data increases inequality and threatens democracy, Great Britain: Penguin Books.</li>
        <li>Jake Silberg and James Manyika. 2020. Tackling bias in artificial intelligence (and in humans). (July 2020). Retrieved from <a href="https://www.mckinsey.com/featured-insights/artificial-intelligence/tackling-bias-in-artificial-intelligence-and-in-humans#">https://www.mckinsey.com/featured-insights/artificial-intelligence/tackling-bias-in-artificial-intelligence-and-in-humans#</a></li>
        <li>Karen Hao. 2020. This is how AI bias really happens-and why it’s so hard to fix. (April 2020). Retrieved from <a href="https://www.technologyreview.com/2019/02/04/137602/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/">https://www.technologyreview.com/2019/02/04/137602/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/</a></li>
        <li>Jeffrey Dastin. 2018. Amazon scraps secret AI recruiting tool that showed bias against women. (October 2018). Retrieved from <a href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G">https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G</a></li>
        <li>Afreen Siddiqi and Babak Heydari. 2019. Equity and Fairness Norms in Sociotechnical Systems: Emerging Perspectives for Design. 2019 IEEE International Symposium on Technology and Society (ISTAS) (2019). DOI:<a href="http://dx.doi.org/10.1109/istas48451.2019.8937878">http://dx.doi.org/10.1109/istas48451.2019.8937878</a></li>
        <li>Emma Pierson Sam Corbett-Davies. 2016. A computer program used for bail and sentencing decisions was labeled biased against blacks. It’s actually not that clear. (October 2016). Retrieved from <a href="https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/"> https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/</a></li>
      </ul>
    </div>
  </div> <!-- row -->
</div>

</body>
</html>
